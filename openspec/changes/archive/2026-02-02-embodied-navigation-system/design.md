## Context

本项目为具身导航系统，通过调用大语言模型（Gemini）的视觉理解能力，实现实时环境感知和路径规划。系统接收视频流输入，抽帧后发送给 LLM 进行分析，输出结构化的导航信息。

当前状态：新项目，无现有代码基础。

约束条件：
- API 端点已提供（支持 Gemini 原生格式和 OpenAI 兼容格式）
- 需要支持摄像头实时流和视频文件两种输入源
- 输出需要可视化展示

## Goals / Non-Goals

**Goals:**
- 实现视频流实时抽帧，保持低延迟
- 支持 Gemini API 的两种调用格式（原生 + OpenAI 兼容）
- 输出结构化的环境分析和任务推理结果
- 生成 5 个导航路径点，支持平滑曲线连接
- 提供直观的可视化界面

**Non-Goals:**
- 不实现完整的 SLAM 系统
- 不处理机器人底层控制（仅输出路径点坐标）
- 不支持多摄像头融合
- 不实现路径点的闭环反馈控制

## Decisions

### D1: 视频抽帧策略 - 单帧队列

**选择**: 使用长度为 1 的队列，新帧覆盖旧帧

**理由**:
- 保证 LLM 始终处理最新帧，避免延迟累积
- 简化内存管理，无需处理帧堆积
- 适合实时导航场景，历史帧无意义

**备选方案**:
- 固定帧率采样：可能错过关键帧
- 多帧缓冲：增加延迟，不适合实时场景

### D2: LLM 客户端架构 - 双格式适配器

**选择**: 实现统一接口，内部适配 Gemini 原生和 OpenAI 兼容两种格式

**理由**:
- 用户可根据需求选择格式
- 便于未来扩展其他 LLM 提供商
- 统一的响应解析逻辑

**备选方案**:
- 仅支持一种格式：灵活性不足
- 完全独立的客户端：代码重复

### D3: 路径点生成 - 底部中心起始 + 等垂直间隔

**选择**:
- 第 1 点：图像底部中心 (width/2, height)
- 第 2-5 点：等垂直间隔向上分布
- 横向位置由 LLM 根据场景分析决定

**理由**:
- 底部中心代表智能体当前位置
- 等间隔便于速度控制
- 横向自由度允许避障

### D4: 可视化方案 - OpenCV + Matplotlib 混合

**选择**:
- 实时预览：OpenCV 窗口
- 分析结果：Matplotlib 叠加层
- 颜色渐变：近点绿色 → 远点红色（HSV 插值）

**理由**:
- OpenCV 适合实时视频处理
- Matplotlib 提供丰富的绘图能力
- HSV 插值产生自然的颜色过渡

### D5: 异步架构

**选择**: 使用 asyncio + 线程池混合模式

**理由**:
- 视频捕获在独立线程（避免阻塞）
- API 调用使用 async/await（高效 IO）
- 主循环协调各模块

## Risks / Trade-offs

**[R1] LLM 响应延迟** → 使用流式响应 + 超时机制，必要时跳帧

**[R2] API 调用失败** → 实现重试逻辑，失败时保持上一帧结果

**[R3] 视频源断开** → 检测断开事件，支持自动重连

**[R4] 结构化输出解析失败** → 使用 JSON Schema 验证，失败时使用默认值

**[R5] 高 GPU/CPU 占用** → 可配置的帧率限制，避免资源耗尽

## Open Questions

- Q1: 是否需要支持多种图像分辨率？当前假设固定分辨率。
- Q2: 路径点的平滑曲线使用什么插值算法？（建议：贝塞尔曲线或样条插值）
- Q3: 是否需要保存分析历史记录？
